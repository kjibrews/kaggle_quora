{
  "cells": [
    {
      "metadata": {
        "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
        "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
        "collapsed": true
      },
      "cell_type": "markdown",
      "source": "# **Quora Insincere Questions Classification**: Detect toxic content to improve online conversations\n\nIn this competition, we aim to classify the Quora dataset found on Kaggle into either insincere or sincere class.\n\nAn insincere question is defined as a question intended to make a statement rather than look for helpful answers. Some characteristics that can signify that a question is insincere:\n- Has a non-neutral tone\n    - Has an exaggerated tone to underscore a point about a group of people\n    - Is rhetorical and meant to imply a statement about a group of people\n- Is disparaging or inflammatory\n    - Suggests a discriminatory idea against a protected class of people, or seeks confirmation of a stereotype\n    - Makes disparaging attacks/insults against a specific person or group of people\n    - Based on an outlandish premise about a group of people\n    - Disparages against a characteristic that is not fixable and not measurable\n- Isn't grounded in reality\n    - Based on false information, or contains absurd assumptions\n- Uses sexual content (incest, bestiality, pedophilia) for shock value, and not to seek genuine answers\n"
    },
    {
      "metadata": {
        "_uuid": "769e45ad1a1b9583ee989fb8d832645b43d08e71"
      },
      "cell_type": "markdown",
      "source": "## 1. Import packages and Pandas\nThe first thing we do is import the necessary packages."
    },
    {
      "metadata": {
        "_uuid": "2e1e074c6eff10d5b16bc17671d3c6a6eabc9d8d",
        "trusted": true
      },
      "cell_type": "code",
      "source": "#import packages and pandas\nimport os\nprint(os.listdir(\"../input\"))\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom matplotlib.offsetbox import AnnotationBbox, OffsetImage\n%matplotlib inline\nimport numpy as np\nimport math as m\nimport time\nimport seaborn as sns\nfrom sklearn import metrics\nfrom tqdm import tqdm #progress bar\nimport gc #garbage collector\n#------------------------------------------------------------------------#\nimport re #regular expression/pattern matching\nimport random\nimport nltk\nimport string\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize \n#------------------------------------------------------------------------#\n#seed for reproducibility\nseed = 12345\nnp.random.seed(seed)\nimport tensorflow as tf\nfrom keras import backend as K\nfrom sklearn.model_selection import train_test_split\n#-------#\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.models import Sequential, Model\nfrom keras.layers import Dense, Flatten, LSTM, Conv1D, MaxPooling1D, Dropout\nfrom keras.layers import Embedding,Concatenate,SpatialDropout1D, Activation\nfrom keras.layers import Reshape,AveragePooling1D,concatenate,Bidirectional\nfrom keras.layers import GlobalMaxPooling1D,GlobalAveragePooling1D, Input,CuDNNGRU\nfrom sklearn.metrics import f1_score, precision_score, recall_score,accuracy_score, classification_report",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "cb1c1757101275898fcbe1ed9aceb64830167297"
      },
      "cell_type": "markdown",
      "source": "## 2. Data\n## 2.1 Data : Import Data\nNext we import the training and testing data.\n- The training data includes the question that was asked, and whether it was identified as insincere (target = 1).   The ground-truth labels contain some amount of noise: they are not guaranteed to be perfect.\n- This competiton adds the following note: \n    - The distribution of questions in the dataset should not be taken to be representative of the distribution of questions asked on Quora. This is, in part, because of the combination of sampling procedures and sanitization measures that have been applied to the final dataset."
    },
    {
      "metadata": {
        "_uuid": "8bb76b68127ff46010691a8ae3f1eb58cf830f0c",
        "trusted": true
      },
      "cell_type": "code",
      "source": "compstart = time.time()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "a57407f5bc9f7e0d96da29843b7d1bceac5a7885",
        "scrolled": true,
        "trusted": true
      },
      "cell_type": "code",
      "source": "#import training and testing data\ntrain = pd.read_csv(\"../input/train.csv\")\ntest = pd.read_csv(\"../input/test.csv\")\nprint(\"Train data shape:\",train.shape)\nprint(\"Test data shape:\", test.shape)\ntrain.head()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_kg_hide-input": false,
        "_kg_hide-output": false,
        "_uuid": "e286d4d7cac32c5716a202e04e1710ebb0804603",
        "trusted": true
      },
      "cell_type": "code",
      "source": "qid =test.qid\n#Check whats in memory\n%who",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "0e1feb9d07f2350d26517357c2d385324a079f6a"
      },
      "cell_type": "markdown",
      "source": "## 2. 2 Data: Explore Data\nNext we explore the data to get a sense of if any transformations or any data cleaning should be done.     \nWe also want to check that the test and train set are similar."
    },
    {
      "metadata": {
        "_uuid": "983abab50e69a6b2fb9b432df66ae91b62702d3b",
        "trusted": false
      },
      "cell_type": "code",
      "source": "#check for missing data\nprint(train[train.isnull().any(axis=1)]) \nprint(test[test.isnull().any(axis=1)])",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_kg_hide-input": true,
        "_uuid": "4301247f90c7266b1e1aa77489311fbd2cca09cb",
        "trusted": false
      },
      "cell_type": "code",
      "source": "#example questions\nprint(\"Sincere Questions:\")\nfor row in np.asarray(train[train[\"target\"] == 0]['question_text'])[0:10]:\n    print(\" \"*3,row)\nprint(\"Insincere Questions:\")\nfor row in np.asarray(train[train[\"target\"] == 1]['question_text'])[0:10]:\n    print(\" \"*3,row)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "c84157cabfab11d5272ed3837009ab21849a7fe5",
        "trusted": false
      },
      "cell_type": "code",
      "source": "def createFeatures(data):\n    #Question Length\n    data[\"quest_len\"] = data.question_text.apply(lambda x: len(x.split()))\n    \n    #English Stopwords\n    eng_stopwords = set(stopwords.words(\"english\"))\n    punc_list = ['\\\\', '?', '.', ';', ',', '-']\n    def nb_stop_words(question):\n        words = re.sub(\"|\".join(punc_list), \"\", question).split(\" \")\n        return len([w for w in words if w in eng_stopwords])\n    data['nb_stop_words'] = data.question_text.apply(nb_stop_words)\n    \n    #Number of character\n    n_charac = pd.Series([len(t) for t in data.question_text])\n    data['n_charac'] = n_charac\n    \n    #Numberof punctuation\n    n_punctuation = pd.Series([sum([1 for x in text if x in set(string.punctuation)]) \n                               for text in data.question_text])\n    data['n_punctuation'] = n_punctuation\n    \n    #Number of uppercase\n    n_upper = pd.Series([sum([1 for c in text if c.isupper()]) for text in data.question_text])\n    data['n_upper'] = n_upper\n    \n    del eng_stopwords\n    gc.collect()\n    return data",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "c9a75ecf83154801c8d12832217186209e948781",
        "trusted": false
      },
      "cell_type": "code",
      "source": "print(\"Creating features for train set\")\nstart = time.time()\ntrain = createFeatures(train)\nend = time.time()\ntle = (end- start) /60\nprint(f\" This process takes {tle} minutes\")\nprint(\"\\nCreating features for test set\")\nstart = time.time()\ntest = createFeatures(test)\nend = time.time()\ntle = (end- start) /60\nprint(f\" This process takes {tle} minutes\")",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "d8cb453c24ed81e407eeaddafe7281a77911383d",
        "trusted": false
      },
      "cell_type": "code",
      "source": "train.head()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "b2e2fa4bcc23920b8153debf5bade90722a0b1c6",
        "trusted": false
      },
      "cell_type": "code",
      "source": "test.head()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_kg_hide-input": true,
        "_uuid": "1ff7a81300b5148b7f68194fd89dbfff14851f81",
        "scrolled": false,
        "trusted": false
      },
      "cell_type": "code",
      "source": "#question length\nprint(\"Feature: Question Length\")\nprint(\"Training Data:\")\nprint(\" \"*3,\"Max:\", max(train.quest_len),\", Min:\", min(train.quest_len))\nprint(\" \"*4,\"Max for sincere target:\",max(train[train.target == 0].quest_len),\n           \", Min for sincere target:\",min(train[train.target == 0].quest_len))\nprint(\" \"*4,\"Max for insincere target:\", max(train[train.target == 1].quest_len),\n           \", Min for insincere target:\",min(train[train.target == 1].quest_len))\nprint(\"\\n Testing Data:\")\nprint(\" \"*3,\"Max:\", max(test.quest_len), \", Min:\", min(test.quest_len))\nprint(\"#---------------------------------------------------------#\")\n#---------------------------------------------------------#\nprint(\"Mean question length:\")\nprint(\"   Training data:\", np.round(np.mean(train.quest_len)))\nprint(\"   Testing data:\", np.round(np.mean(test.quest_len)))\nprint(\"\\n Mean question length:\")\nprint(\"   Sincere question\", np.round(np.mean(np.asarray(train[train.target == 0].quest_len))))\nprint(\"   Insincere question\", np.round(np.mean(np.asarray(train[train.target == 1].quest_len))))\n#---------------------------------------------------------#\n#Distribution of question length color coded by Question type\nfig, ax = plt.subplots(1,2,figsize=(20,10))\nsns.countplot(train[train.target == 0].quest_len,ax=ax[0]).set_title(\"Questions Length Distribution for Sincere\")\nsns.countplot(train[train.target == 1].quest_len,ax=ax[1]).set_title(\"Questions Length Distribution for Insincere\")\nplt.tight_layout()\nplt.show()\n#---------------------------------------------------------#\nfig, ax =plt.subplots(1,2,figsize=(20,10))\nsns.countplot(train.quest_len,ax=ax[0]).set_title(\"Questions Length Distribution for Train\")\nsns.countplot(test.quest_len,ax=ax[1]).set_title(\"Questions Length Distribution for Test\")\nplt.tight_layout()\nplt.show()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_kg_hide-input": true,
        "_uuid": "da59ffe51fac25ab03b214a0fbf0f253a1fb0a3f",
        "scrolled": false,
        "trusted": false
      },
      "cell_type": "code",
      "source": "#stopwords\nprint(\"Feature: Number of stopwords\")\n#Out of curiosity. questions without stopwords\nprint(\"Examples of questions without stopwords:\")\nfor row in train[train.nb_stop_words==0].question_text[0:5]:\n    print(\" \"*3, row)\n#----------------------------------------------------------------#\nprint(\"#----------------------------------------------------------#\")\nprint(\"Mean # of stopwords in training data:\", np.round(np.mean(train.nb_stop_words)))\nprint(\"Mean # of stopwords in testing data:\", np.round(np.mean(test.nb_stop_words)))\nprint(\"#----------------------------------------------------------#\")\nprint(\"Number of stopwords per sentence\")\nprint(\" Training Data:\")\nprint(\"  Max # in train:\" ,max(train.nb_stop_words),\"; Min # in train:\" ,min(train.nb_stop_words))\nprint(\"   Sincere target: \", \"Max:\", max(train[train.target == 0].nb_stop_words),\n      \"; Min:\", min(train[train.target == 0].nb_stop_words))\nprint(\"   Insincere target: \", \"Max:\", max(train[train.target == 1].nb_stop_words),\n      \"; Min:\", min(train[train.target == 1].nb_stop_words))      \nprint(\" \\n Training Data:\")\nprint(\"Max # in test:\", min(test.nb_stop_words),\"; Min # in test:\", max(test.nb_stop_words))\n#----------------------------------------------------------------#\nfig, ax =plt.subplots(1,2,figsize=(20, 10))\nsns.countplot(train.nb_stop_words,ax=ax[0]).set_title(\"Number of Stopwords for Train\")\nsns.countplot(test.nb_stop_words,ax=ax[1]).set_title(\"Number of Stopwords for Test\")\n#----------------------------------------------------------------#\nfig, ax =plt.subplots(1,2,figsize=(20, 10))\nsns.countplot(train[train.target == 0].nb_stop_words,ax=ax[0]).set_title(\"Number of Stopwords per Sentence by Sincere\")\nsns.countplot(train[train.target == 1].nb_stop_words).set_title(\"Number of Stopwords per Sentence by Insincere\")\nplt.show()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "275899af2ce84b73740b03f2f5bae189befc4cc6",
        "trusted": false
      },
      "cell_type": "code",
      "source": "print(\"Top 25 common amd rare words\")\ncomm_word = pd.Series(' '.join(train.question_text).split()).value_counts()[:25]\ncomm_word = pd.DataFrame({'word':comm_word.index, 'frequency':comm_word.values})\nrare_word = pd.Series(' '.join(train.question_text).split()).value_counts()[-25:]\nrare_word = pd.DataFrame({'word':rare_word.index, 'frequency':rare_word.values})\nfig, ax =plt.subplots(1,2,figsize=(20, 10))\nplt.gca().invert_yaxis()\nax[0].set_title(\"Top 25 Common Words\", fontsize = 14)\nax[0].barh(comm_word.word,comm_word.frequency)\nax[0].invert_yaxis()\nfor i, v in enumerate(comm_word.frequency):\n    ax[0].text(v + 3, i + .25, str(v), color='blue', fontweight='bold')\n#\nax[1].set_title(\"Top 25 Rare Words\",fontsize = 14)\nax[1].barh(rare_word.word,rare_word.frequency)\nplt.tight_layout()\nplt.show()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "9a8fa9bc8d86381d64e6f8972a454ca17c1b8203"
      },
      "cell_type": "markdown",
      "source": "The above plot shows that the before we can access common and rare words we need to clean the data first, most importantly remove puntuation."
    },
    {
      "metadata": {
        "_uuid": "c7958485450abd461fbcabd219a8c667b20e4bc1"
      },
      "cell_type": "markdown",
      "source": "CountVectorizer counts the word frequencies.\nWith TFIDFVectorizer the value increases proportionally to count, but is offset by the frequency of the word in the corpus (i.e. the training set). This is the IDF (inverse document frequency part). This helps to adjust for the fact that some words appear more frequently.\nTherefore we could use TFID to remove words that have a document frequency strictly lower than a certain value. This is set through the parameter min_df.\n\nIn a corpus, several common words makes up lot of space which carry very little information about content of document. If we feed these counts directly to a classifier then those frequently occurring words will shadow the real interesting terms of the document. So we re-weight count feature vectors using tf-idf transform method and then feed the data into classifier for better classification.\nTfidfVectorizer combines all options of CountVectorizer and TfidfTransformer in a single model."
    },
    {
      "metadata": {
        "_uuid": "44193ef5706bd735dda24b7cb7a6836762478fc2",
        "trusted": false
      },
      "cell_type": "code",
      "source": "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\ntarg0_data = train[train.target == 0].question_text\ntarg1_data = train[train.target == 1].question_text\n\n# create the object of tfid vectorizer for each class\nSin_cv_vectorizer = CountVectorizer().fit(targ0_data) \nprint('Vocabulary len of Sincere:', len(Sin_cv_vectorizer.get_feature_names()))\nprint('Shortest word in Sincere:', min(Sin_cv_vectorizer.vocabulary_, key=len))\nprint('Longest word in Sincere:', max(Sin_cv_vectorizer.vocabulary_, key=len))\n\nInSin_cv_vectorizer = CountVectorizer().fit(targ1_data)\nprint('\\nVocabulary len of Insincere:', len(InSin_cv_vectorizer.get_feature_names()))\nprint('Shortest word in Sincere:', min(InSin_cv_vectorizer.vocabulary_, key=len))\nprint('Longest word in Insincere:', max(InSin_cv_vectorizer.vocabulary_, key=len))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "f194236dee010d59dba6504551f4796df0573832",
        "trusted": false
      },
      "cell_type": "code",
      "source": "cvSin_dictionary = Sin_cv_vectorizer.vocabulary_.items()\ncvInSin_dictionary = InSin_cv_vectorizer.vocabulary_.items()\n# lists to store the vocab and counts\ncvSinvocab = []\ncvSincount = []\n    # iterate through each vocab and count append the value to designated lists\nfor Sinkey, Sinvalue in cvSin_dictionary:\n    cvSinvocab.append(Sinkey)\n    cvSincount.append(Sinvalue)\n    \n# lists to store the vocab and counts\ncvInSinvocab = []\ncvInSincount = []\n    # iterate through each vocab and count append the value to designated lists\nfor InSinkey, InSinvalue in cvInSin_dictionary:\n    cvInSinvocab.append(InSinkey)\n    cvInSincount.append(InSinvalue)\n\n# store the count in panadas dataframe with vocab as index\ncvSin_vocab = pd.Series(cvSincount, index=cvSinvocab).sort_values(ascending=False)\ncvInSin_vocab = pd.Series(cvInSincount, index=cvInSinvocab).sort_values(ascending=False)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_kg_hide-input": true,
        "_uuid": "1bf0cbff2769cac9b4bd720dc842455db0b2f02d",
        "trusted": false
      },
      "cell_type": "code",
      "source": "print(\"Feature: TFIDF Words\")\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n# create the object of tfid vectorizer\nSin_tfid_vectorizer = TfidfVectorizer(\"english\").fit(targ0_data)\nInSin_tfid_vectorizer = TfidfVectorizer(\"english\").fit(targ1_data)\n\n# collect the vocabulary items used in the vectorizer\nSin_dictionary = Sin_tfid_vectorizer.vocabulary_.items()\nInSin_dictionary = InSin_tfid_vectorizer.vocabulary_.items()\n\n# lists to store the vocab and counts\nSinvocab = []\nSincount = []\n    # iterate through each vocab and count append the value to designated lists\nfor Sinkey, Sinvalue in Sin_dictionary:\n    Sinvocab.append(Sinkey)\n    Sincount.append(Sinvalue)\n    \n# lists to store the vocab and counts\nInSinvocab = []\nInSincount = []\n    # iterate through each vocab and count append the value to designated lists\nfor InSinkey, InSinvalue in InSin_dictionary:\n    InSinvocab.append(InSinkey)\n    InSincount.append(InSinvalue)\n\n# store the count in panadas dataframe with vocab as index\nSin_vocab = pd.Series(Sincount, index=Sinvocab).sort_values(ascending=False)\nInSin_vocab = pd.Series(InSincount, index=InSinvocab).sort_values(ascending=False)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_kg_hide-input": true,
        "_uuid": "5c1b8a31c1d6cb5a664b8ea5bfad534e9a147881",
        "trusted": false
      },
      "cell_type": "code",
      "source": "Sintop_vacab = Sin_vocab.head(30)\nSintop_vacab = pd.DataFrame({'word':Sintop_vacab.index, 'frequency':Sintop_vacab.values})\nInSintop_vacab = InSin_vocab.head(30)\nInSintop_vacab = pd.DataFrame({'word':InSintop_vacab.index, 'frequency':InSintop_vacab.values})\n# plots of the top vocab\nfig, ax =plt.subplots(1,2,figsize=(20, 10))\nax[0].set_title(\"Top 20 TFID Words in Sincere Questions\", fontsize = 14)\nax[0].barh(Sintop_vacab.word,Sintop_vacab.frequency)\nax[1].set_title(\"Top 20 TFID Words in Insincere Questions\",fontsize = 14)\nax[1].barh(InSintop_vacab.word,InSintop_vacab.frequency)\nplt.tight_layout()\nplt.show()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "e54b0a57aba9e879ef141297a012e993f9349c21",
        "scrolled": true,
        "trusted": false
      },
      "cell_type": "code",
      "source": "print(f\"Length of Sincere TFID vocab: {len(Sin_vocab)}\")\nprint(Sin_vocab.head(75))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "2a429bbfa7f9b77f69c213dced7e769dcb60a5b1"
      },
      "cell_type": "markdown",
      "source": "Contains words:  to, Korean  ,happily ,black hair ,Korean   "
    },
    {
      "metadata": {
        "_uuid": "e8d2c778da6edf159db4631d093a33b7391e12cb",
        "trusted": false
      },
      "cell_type": "code",
      "source": "print(f\"Length of Sincere TFID vocab: {len(InSin_vocab)}\")\nprint(InSin_vocab.head(75))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "3eb29f279ebcd80e8aa7cb0aa2c0b2f72d9f4b6b"
      },
      "cell_type": "markdown",
      "source": "This contains weird fonts and other languages:   (translations made using Goofle translate\n    韓国人 => Japanese: Korean person  \n    素质  => Chinese: Quality  \n    管中閔       Chinese: In the tube   \n    福哒柄          Chinese: good fortune  \n    海南人の日本       Japenese: Hainan people's Japan  \n    操你妈        Chinese: F*** your mother    \n    安倍晋三         Japenese; Shinzo Abe (PM of Japan)  \n    在日朝鮮人       Chinese: Koreans in Japan    \n    不正常人類研究中心    Chinese: Abnormal human research center  \n    ṭaiyibah     => Hindi/Islamic : purity  \n    ᗰoᖇe      => English : more #zigjaw/1 font ?  \n    ᗰeᑎ         => English men   \n    ᗯᕼy          => English why  \n    ᗯoᗰeᑎ        => English women   \n    ᗩᖇe         => English are   \n    ᗩtteᑎtiᐯe English attentive  \n    ਜਬ           => Punjabi: when  \n    रत            => Punjabi: the  \n    पन           => Punjabi: water  \n    चमत          => Punjabi: Excitement  \n"
    },
    {
      "metadata": {
        "_kg_hide-input": true,
        "_uuid": "93a2b42f452e58764fd4f8d4db05bd75c9d25e1a",
        "trusted": false
      },
      "cell_type": "code",
      "source": "# plot of the least frequent vocab\nSinlow_vacab = Sin_vocab.tail(20)\nSinlow_vacab = pd.DataFrame({'word':Sinlow_vacab.index, 'frequency':Sinlow_vacab.values})\nInSinlow_vacab = InSin_vocab.tail(20)\nInSinlow_vacab = pd.DataFrame({'word':InSinlow_vacab.index, 'frequency':InSinlow_vacab.values})\n# plots of the top vocab\nfig, ax =plt.subplots(1,2,figsize=(20, 10))\nax[0].set_title(\"Least Frequent 20 TFID  Words in Sincere Questions\", fontsize = 14)\nax[0].barh(Sinlow_vacab.word,Sinlow_vacab.frequency)\nax[1].set_title(\"Least Frequent 20 TFID Words in Insincere Questions\",fontsize = 14)\nax[1].barh(InSinlow_vacab.word,InSinlow_vacab.frequency)\nplt.tight_layout()\nplt.show()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "a41aafecd94e61ac5f6df3ec0949cbd149a417fe"
      },
      "cell_type": "markdown",
      "source": "This plot shows that these questions contain non-English words and numbers ."
    },
    {
      "metadata": {
        "_uuid": "29d1c4f188d9a9c5a00676b1273c12163f4469aa",
        "trusted": false
      },
      "cell_type": "code",
      "source": "print(\"Feature: Target\")\n#target_count = target.value_counts()\nplt.figure(figsize=(8,8))\ntrain.target.value_counts().plot(kind=\"bar\")\nplt.xticks((0, 1), (\"Sincere\", \"Insincere\"), rotation=0, fontsize=14)\nplt.yticks(fontsize=14)\nplt.title(\"Target Count by Class\",fontsize=20)\nplt.xlabel(\"Target\",fontsize=14)\nplt.ylabel(\"Count\",fontsize=14)\nplt.show()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "c41e00c83d962c8984891dd95f75e9fd58e9287c",
        "trusted": false
      },
      "cell_type": "code",
      "source": "print(\"Sincere target is\",np.round((train.target.value_counts()/len(train.target))[0] *100 ,3), \"% of training set.\")\nprint(\"Insincere target is\",np.round((train.target.value_counts()/len(train.target))[1] *100 ,3), \"% of training set.\")",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "81cda87681299cb4c17cca1ff12292ebd23fd61f"
      },
      "cell_type": "markdown",
      "source": "### Results from Data Exploration:\n- No missing questions.\n- Questions do contain contractions, need to clean.\n- Insincere questions are shorter in question length than sincere question.\n- Should stopwords be removed: Note that the pre trained stopword dictionary is not the best. Removing stopwords may not be the best approach.\n- The majority of the questions in both data sets are  $\\approx 10-15$ words long (i.e. excluding punctuation).\n- Insincere questions are shorter than sincere questions on average.\n- The target is imbalanced so we will have to deal with this.\n"
    },
    {
      "metadata": {
        "_kg_hide-input": true,
        "_uuid": "e9273a5e414c648cb7a8bd922e0013fec3b1d104",
        "trusted": false
      },
      "cell_type": "code",
      "source": "train.head()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_kg_hide-input": true,
        "_uuid": "d87fed8a9caf346cfb091b5e0a482f35fe0ba920",
        "trusted": false
      },
      "cell_type": "code",
      "source": "#not necessary in classification,\ndel train[\"quest_len\"], test[\"quest_len\"], train['nb_stop_words'] ,test['nb_stop_words']\ngc.collect()\ntime.sleep(10)\ntrain.head()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_kg_hide-input": true,
        "_uuid": "8211fc791af70f87fbf9c43f06cbc8d47a01f8c2",
        "trusted": false
      },
      "cell_type": "code",
      "source": "del train[\"n_charac\"], test[\"n_charac\"], train['n_punctuation'] ,test['n_punctuation'],train[\"n_upper\"], test[\"n_upper\"]\ngc.collect()\ntime.sleep(10)\ntrain.head()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "2f6baff94f182b567f45791e9ded1969acc77ed1",
        "trusted": false
      },
      "cell_type": "code",
      "source": "del cvSinvocab,cvSincount,cvSin_vocab,cvSin_dictionary,Sinvalue,Sincount,Sintop_vacab,Sinlow_vacab\ngc.collect()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "c251d1c1f41d3b47da65c6f5a79b4eff50420c9c",
        "trusted": false
      },
      "cell_type": "code",
      "source": "del fig, ax, InSinlow_vacab, cvInSin_vocab, cvInSin_dictionary, cvInSincount, targ0_data,targ1_data\ngc.collect()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "a24e5ef5df6a3e1d4c19478993288e360d589419",
        "trusted": false
      },
      "cell_type": "code",
      "source": "del Sin_dictionary,Sin_tfid_vectorizer,InSin_vocab,InSincount,InSinkey,InSintop_vacab,InSinvalue,InSinvocab, createFeatures",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "4ebc6a0f92661424b183627423213c4ca952edb3",
        "trusted": false
      },
      "cell_type": "code",
      "source": "del InSin_cv_vectorizer,Sin_cv_vectorizer,cvInSinvocab, InSin_dictionary,  InSin_tfid_vectorizer, Sin_vocab, Sinkey,Sinvocab",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_kg_hide-input": true,
        "_kg_hide-output": true,
        "_uuid": "08d58536acbd7bd9cd8fb16cb850ac9e2fc7ed9c",
        "trusted": false
      },
      "cell_type": "code",
      "source": "#Check whats in memory\ngc.collect()\ntime.sleep(5)\n%who",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "2ad21e96c37e1bef21a04f8b75f6af612dba7302"
      },
      "cell_type": "markdown",
      "source": "## 3. Data Cleaning\nWe will clean the data by doing the following:\n- Replace contractions: replace all contractions with the full meaning. (Contractions scrapped from Wikipedia: https://en.wikipedia.org/wiki/Wikipedia:List_of_English_contractions)\n- Change case: change case of the question text:  transform question text into lower upper case.\n- Lemmatisation of questions.\n- Punctuation removal : remove all instances of punctuation.\n- Replace spelling errors: replace misspelled words with the correct spelling.  (Common misspellings  scrapped from oxford dictionaries: https://en.oxforddictionaries.com/spelling/common-misspellings)\n- Remove common and frequent words."
    },
    {
      "metadata": {
        "_kg_hide-input": true,
        "_kg_hide-output": false,
        "_uuid": "ce9982d0476a1f11a64347c326550968ee7f5e47",
        "trusted": false
      },
      "cell_type": "code",
      "source": "#Used this code, NOT RUN IN KERNEL to get common contractions from wikipedia \n#from urllib.request import urlopen\n#from bs4 import BeautifulSoup\n#def getContractionDict():\n#    url = \"https://en.wikipedia.org/wiki/Wikipedia:List_of_English_contractions\"\n#    #functions\n#    def getHTMLContent(link):\n#        html = urlopen(link)\n#        soup = BeautifulSoup(html, 'html.parser')\n#        return soup\n#\n#    def getText(rows):\n#        for i in range(1,len(rows)-1):\n#            rows[i]  = rows[i].text.split(\"\\n\")\n#        return rows\n\n#   def remBrackInfo(Textrows):\n#        for i in range(0,len(Textrows)-1):\n#           fld0 = Textrows[i][0] \n#            fld1 = Textrows[i][1]\n#            fld0 = fld0.split(\"(\")[0].strip()\n#            fld0 = fld0.split(\"[\")[0].strip()\n#            fld1 = fld1.split(\"(\")[0].strip() #+ fld.split(\")\")[1]\n#            fld1 = fld1.split(\"[\")[0].strip() #+ fld.split(\"]\")[1]\n#            Textrows[i][0] = fld0\n#            Textrows[i][1] = fld1\n#       return Textrows\n\n#    def oneMeaning(Textrows):\n#        for i in range(0,len(Textrows)-1):\n#            var = Textrows[i][1]\n#            var = var.split(\"/\")[0]\n#            Textrows[i][1] = var\n#        return Textrows\n    #------------------------------------------------------------------------#\n    \n#    content = getHTMLContent(url)\n#    tables = content.find_all(\"table\")\n#    table = content.find('table', {'class': 'wikitable sortable'})\n#    rows = table.find_all('tr')\n#    Trows = getText(rows)[1:]\n#    Trows = remBrackInfo(Trows)\n#    Trows = oneMeaning(Trows)\n##    Trows = Trows[:-1]\n#    arr= dict()\n#    for item in Trows:\n#        arr[item[0]] = item[1]\n#    return arr ",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_kg_hide-input": true,
        "_kg_hide-output": false,
        "_uuid": "9f48589b9f34cdbb24b399ee5de1ee0085b3ca0e",
        "trusted": false
      },
      "cell_type": "code",
      "source": "#Used this code, NOT RUN IN KERNEL to get common misspellings from oxforddictioanry website\n#import numpy as np\n#import pandas as pd\n\n#from urllib.request import urlopen\n#from bs4 import BeautifulSoup\n\n#def getCommMispellDict():\n#    url = \"https://en.oxforddictionaries.com/spelling/common-misspellings\"\n#        #functions\n#    def getHTMLContent(link):\n#        html = urlopen(link)\n#        soup = BeautifulSoup(html, 'html.parser')\n#        return soup\n\n#    def CorrWords():\n#        correct = []\n#        for link in links:\n#            correct.append(link.text.strip())\n\n        #Some words are not linked, manually add them\n        #Non linked and previous\n#        correct.insert(correct.index(\"millennium\") + 1,\"millennia\")\n#        correct.insert(correct.index(\"occurred\")  + 1,\"occurring\")\n#        correct.insert(correct.index(\"preferred\")  + 1,\"preferring\")\n#        correct.insert(correct.index(\"referred\")  + 1,\"referring\")\n#        correct.insert(correct.index(\"believe\")  + 1,\"believe\")\n#        correct.insert(correct.index(\"occurrence\")  + 1,\"occurrence\")\n#        correct.insert(correct.index(\"occasion\")  + 1,\"occasion\")\n#        correct.insert(correct.index(\"tomorrow\")  + 1,\"tomorrow\")\n#        correct.insert(correct.index(\"remember\")  + 1,\"remember\")\n#        return correct\n\n#    def MissWords():\n#        misspelled_words = table.find_all(\"td\")[3:]\n#        misspell = []\n#        for i in range(2,len(misspelled_words), 3):\n#            misspell.append(misspelled_words[i])\n#\n#        for i in range(0,len(misspell)):\n#            misspell[i]  = misspell[i].text.split(\" \")\n#\n#        cnt = 0\n#        val = []\n#        for i in misspell:\n#            for element in i:\n#                parts = element.split(\" \")[0].strip(\",\")\n#                cnt +=1\n#                val.append(parts)\n#        return val\n\n\n#    content = getHTMLContent(url)\n#    tables = content.find_all(\"table\")\n#    table = content.find('table')\n#    links = table.find_all('a')\n#    Keys = CorrWords()\n#    Values = MissWords()\n\n\n#    return dict(zip(Keys,Values))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "eff3522dea15c7b7dcba94f05dae995816e6382a",
        "trusted": true
      },
      "cell_type": "code",
      "source": "#SYNONYM SUBSTITUTION FUNCTION (not effective so not used in final model)\ndef synonym_transform(text):\n    text = str(text)\n    text = text.lower()\n        # A Synonym for common english words.\n    # source: https://justenglish.me/2014/04/18/synonyms-for-the-96-most-commonly-used-words-in-english/\n    synonym_dict = {'amazing':'incredible', 'anger':'enrage', 'angry':'mad', 'answer':'reply', 'ask':'question', 'awful':'dreadful',\n        'bad':'evil', 'beautiful':'pretty', 'begin':'start', 'big':'large', 'brave':'courageous', 'break':'fracture', 'bright':'shining',\n        'calm':'quiet', 'come':'approach', 'cool':'chilly', 'crooked':'bent', 'cry':'shout', 'cut':'slash', 'dangerous':'perilous',\n        'dark':'dim', 'decide':'determine', 'definite':'certain', 'delicious':'savory', 'describe':'portray', 'destroy':'ruin', 'difference':'contrast',\n        'dull':'boring', 'eager':'enthusiastic', 'end':'stop', 'enjoy':'appreciate', 'explain':'clarify', 'fair':'just', 'fall':'drop', 'false':'fake',\n        'famous':'renowned', 'fast':'quick', 'fat':'stout', 'fear':'terror', 'fly':'soar', 'funny':'amusing', 'get':'acquire', 'go':'recede', \n        'good':'excellent', 'great':'noteworthy', 'gross':'improper', 'happy':'pleased', 'hate':'despise', 'have':'hold', 'help':'assist', 'hide':'conceal',\n        'hurry':'rush', 'hurt':'damage', 'idea':'thought', 'important':'necessary', 'interesting':'fascinating', 'keep':'hold', 'kill':'murder', 'lazy':'inactive',\n        'little':'small', 'look':'gaze', 'love':'admire', 'make':'create', 'mark':'label', 'mischievous':'playful', 'move':'travel', 'moody':'irritable',\n        'neat':'clean', 'new':'fresh', 'old':'ancient', 'part':'portion', 'place':'space', 'plan':'plot', 'popular':'celecrated', 'predicament':'dilemma',\n        'put':'set', 'quiet':'silent', 'run':'sprint', 'say':'inform', 'tell':'advise', 'scared':'afraid', 'show':'display', 'slow':'gradual', 'stop':'cease',\n        'story':'tale', 'strange':'odd', 'take':'seize', 'think':'believe', 'trouble':'distress', 'true':'accurate', 'ugly':'horrible', 'unhapy':'miserable',\n        'use':'employ', 'wrong':'incorrect', 'aggressive':'mean', 'rude':'mean', 'defend':'fight for', 'unreasonable':'irrational', 'crazy':'insane',\n        'violent':'savage', 'hater':'doubter', 'haters':'doubters', 'weak':'feeble', 'fool':'idiot', 'fools':'idiots', 'obese':'fat',\n        'dislike':'hate', 'hatred':'disgust'}\n    \n    for key in synonym_dict.keys():\n        text = text.replace(key, synonym_dict[key])   \n    return text",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_kg_hide-input": true,
        "_uuid": "fa68e5315e5bf2968fd434124ff5e0b090dc4e03",
        "trusted": true
      },
      "cell_type": "code",
      "source": "#ContDict = getContractionDict()\nContDict = {\"ain't\": 'am not ', \"amn't\": 'am not', \"aren't\": 'are not', \"can't\": 'cannot', \"'cause\": 'because', \"could've\": 'could have', \n            \"couldn't\": 'could not', \"couldn't've\": 'could not have', \"daren't\": 'dare not ', \"daresn't\": 'dare not', \"dasn't\": 'dare not',\n            \"didn't\": 'did not', \"doesn't\": 'does not', \"don't\": 'do not ', \"e'er\": 'ever', \"everyone's\": 'everyone is', 'finna': 'fixing to',\n            'gimme': 'give me', 'gonna': 'going to', \"gon't\": 'go not', 'gotta': 'got to', \"hadn't\": 'had not', \"hasn't\": 'has not', \n            \"haven't\": 'have not', \"he'd\": 'he had ', \"he'll\": 'he shall ', \"he's\": 'he has ', \"he've\": 'he have', \"how'd\": 'how did ',\n            \"how'll\": 'how will', \"how're\": 'how are', \"how's\": 'how has ', \"I'd\": 'I had ', \"I'll\": 'I shall ', \"I'm\": 'I am', \n            \"I'm'a\": 'I am about to', \"I'm'o\": 'I am going to', \"I've\": 'I have', \"isn't\": 'is not', \"it'd\": 'it would', \"it'll\": 'it shall ',\n            \"it's\": 'it has ', \"let's\": 'let us', \"mayn't\": 'may not', \"may've\": 'may have', \"mightn't\": 'might not', \"might've\": 'might have',\n            \"mustn't\": 'must not', \"mustn't've\": 'must not have', \"must've\": 'must have', \"needn't\": 'need not', \"ne'er\": 'never',\n            \"o'clock\": 'of the clock', \"o'er\": 'over', \"ol'\": 'old', \"oughtn't\": 'ought not', \"'s\": 'is, has, does, or us',\n            \"shalln't\": 'shall not', \"shan't\": 'shall not', \"she'd\": 'she had ', \"she'll\": 'she shall ', \"she's\": 'she has ',\n            \"should've\": 'should have', \"shouldn't\": 'should not', \"shouldn't've\": 'should not have', \"somebody's\": 'somebody has ',\n            \"someone's\": 'someone has ', \"something's\": 'something has ', \"that'll\": 'that shall ', \"that're\": 'that are',\n            \"that's\": 'that has ', \"that'd\": 'that would ', \"there'd\": 'there had ', \"there'll\": 'there shall ', \"there're\": 'there are', \n            \"there's\": 'there has ', \"these're\": 'these are', \"they'd\": 'they had ', \"they'll\": 'they shall ', \"they're\": 'they are ', \n            \"they've\": 'they have', \"this's\": 'this has ', \"those're\": 'those are', \"'tis\": 'it is', \"'twas\": 'it was', \"wasn't\": 'was not',\n            \"we'd\": 'we had ', \"we'd've\": 'we would have', \"we'll\": 'we will', \"we're\": 'we are', \"we've\": 'we have', \"weren't\": 'were not',\n            \"what'd\": 'what did', \"what'll\": 'what shall ', \"what're\": 'what are', \"what's\": 'what has ', \"what've\": 'what have', \"when's\": 'when has ',\n            \"where'd\": 'where did', \"where're\": 'where are', \"where's\": 'where has ', \"where've\": 'where have', \"which's\": 'which has ',\n            \"who'd\": 'who would ', \"who'd've\": 'who would have', \"who'll\": 'who shall ', \"who're\": 'who are', \"who's\": 'who has ', \"who've\": 'who have', \n            \"why'd\": 'why did', \"why're\": 'why are', \"why's\": 'why has ', \"won't\": 'will not', \"would've\": 'would have', \"wouldn't\": 'would not',\n            \"y'all\": 'you all', \"you'd\": 'you had ', \"you'll\": 'you shall ', \"you're\": 'you are', \"you've\": 'you have', \"noun's\": 'noun is'}\n\nPunctDict = {\"‘\": \"'\", \"₹\": \"e\", \"´\": \"'\", \"°\": \"\", \"€\": \"e\", \"™\": \"tm\", \"√\": \" sqrt \", \"×\": \"x\", \"²\": \"2\", \"—\": \"-\", \"–\": \"-\", \n                 \"’\": \"'\", \"_\": \"-\", \"`\": \"'\", '“': '\"', '”': '\"', '“': '\"', \"£\": \"e\", '∞': 'infinity', 'θ': 'theta', '÷': '/', 'α': 'alpha', \n                 '•': '.', 'à': 'a', '−': '-', 'β': 'beta', '∅': '', '³': '3', 'π': 'pi', }    \nCom_MisspellDict = {'colour': 'color', 'centre': 'center', 'favourite': 'favorite', 'travelling': 'traveling', \n                    'counselling': 'counseling', 'theatre': 'theater', 'cancelled': 'canceled', 'labour': 'labor',\n                    'organisation': 'organization', 'wwii': 'world war 2', 'citicise': 'criticize', \n                    'youtu ': 'youtube ', 'Qoura': 'Quora', 'sallary': 'salary', 'Whta': 'What',\n                    'narcisist': 'narcissist', 'howdo': 'how do', 'whatare': 'what are', \n                    'howcan': 'how can', 'howmuch': 'how much', 'howmany': 'how many',\n                    'whydo': 'why do', 'doI': 'do I', 'theBest': 'the best', 'howdoes': 'how does', \n                    'mastrubation': 'masturbation', 'mastrubate': 'masturbate', \"mastrubating\": 'masturbating',\n                    'pennis': 'penis', 'Etherium': 'Ethereum', 'narcissit': 'narcissist', 'bigdata': 'big data', \n                    '2k17': '2017', '2k18': '2018', 'qouta': 'quota', 'exboyfriend': 'ex boyfriend', \n                    'airhostess': 'air hostess', \"whst\": 'what', 'watsapp': 'whatsapp', 'demonitisation': 'demonetization',\n                    'demonitization': 'demonetization', 'demonetisation': 'demonetization', 'pokémon': 'pokemon'}\npunc_list = ['\\\\', '?', '.', ';', ',', '-']\n\n#Oxfd_MisspellDict = getCommMispellDict()\nOxfd_MisspellDict = {'accommodate': 'accomodate', 'accommodation': 'accomodation', 'achieve': 'acheive', \n 'across': 'accross', 'aggressive': 'agressive', 'aggression': 'agression', \n 'apparently': 'apparantly', 'appearance': 'appearence', 'argument': 'arguement',\n 'assassination': 'assasination', 'basically': 'basicly', 'beginning': 'begining',\n 'believe': 'belive', 'bizarre': 'bizzare', 'business': 'buisness', 'calendar': 'calender',\n 'Caribbean': 'Carribean', 'cemetery': 'cemetary', 'chauffeur': 'chauffer', 'colleague': 'collegue',\n 'coming': 'comming', 'committee': 'commitee', 'completely': 'completly', 'conscious': 'concious',\n 'curiosity': 'curiousity', 'definitely': 'definately', 'dilemma': 'dilemna', 'disappear': 'dissapear',\n 'disappoint': 'dissapoint', 'ecstasy': 'ecstacy', 'embarrass': 'embarass', 'environment': 'enviroment',\n 'existence': 'existance', 'Fahrenheit': 'Farenheit', 'familiar': 'familar', 'finally': 'finaly',\n 'fluorescent': 'florescent', 'foreign': 'foriegn', 'foreseeable': 'forseeable', 'forty': 'fourty',\n 'forward': 'foward', 'friend': 'freind', 'further': 'futher', 'gist': 'jist', 'glamorous': 'glamourous',\n 'government': 'goverment', 'guard': 'gaurd', 'happened': 'happend', 'harass': 'harrass', \n 'harassment': 'harrassment', 'honorary': 'honourary', 'humorous': 'humourous', 'idiosyncrasy': 'idiosyncracy',\n 'immediately': 'immediatly', 'incidentally': 'incidently', 'independent': 'independant',\n 'interrupt': 'interupt', 'irresistible': 'irresistable', 'knowledge': 'knowlege', 'liaise': 'liase',\n 'liaison': 'liason', 'lollipop': 'lollypop', 'millennium': 'millenium', 'millennia': 'millenia',\n 'Neanderthal': 'Neandertal', 'necessary': 'neccessary', 'noticeable': 'noticable', 'occasion': 'occassion',\n 'occurred': 'occured', 'occurring': 'occuring', 'occurrence': 'occurence', 'pavilion': 'pavillion', \n 'persistent': 'persistant', 'pharaoh': 'pharoah', 'piece': 'peice', 'politician': 'politican',\n 'Portuguese': 'Portugese', 'possession': 'posession', 'preferred': 'prefered', 'preferring': 'prefering',\n 'propaganda': 'propoganda', 'publicly': 'publically', 'really': 'realy', 'receive': 'recieve', \n 'referred': 'refered', 'referring': 'refering', 'religious': 'religous', 'remember': 'remeber', \n 'resistance': 'resistence', 'sense': 'sence', 'separate': 'seperate', 'siege': 'seige', \n 'successful': 'succesful', 'supersede': 'supercede', 'surprise': 'suprise', 'tattoo': 'tatoo', \n 'tendency': 'tendancy', 'therefore': 'therefor', 'threshold': 'threshhold', 'tomorrow': 'tommorrow',\n 'tongue': 'tounge', 'truly': 'truely', 'unforeseen': 'unforseen', 'unfortunately': 'unfortunatly',\n 'until': 'untill', 'weird': 'wierd', 'wherever': 'whereever', 'which': 'wich'}",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "54c16892de5c83166c47406a8258092af6a424ab",
        "trusted": true
      },
      "cell_type": "code",
      "source": "#np.asarray(list(punct_mapping.values()))\nprint(len(Com_MisspellDict),len(Oxfd_MisspellDict))\nMisspellDict = {**Oxfd_MisspellDict, **Com_MisspellDict}\nprint(len(MisspellDict))\ndel Oxfd_MisspellDict,Com_MisspellDict",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "60d11e849c98eb739b121be7f2715723702b29cc",
        "trusted": true
      },
      "cell_type": "code",
      "source": "def  Replc(x):\n    \"This fuction remove contractions, replace unknown characters with known characters, replace commonly misspelled words, removes punctuation\"\n    for dic in [ContDict, PunctDict,MisspellDict,PunctDict]: \n        for word in dic.keys():\n            x = x.replace(word, dic[word])\n    for p in punc_list:\n        x = x.replace(p, f' {p} ')\n             \n    specials = {'\\u200b': ' ', '…': ' ... ', '\\ufeff': '', 'करना': '', 'है': ''}  #special characters last\n    for s in specials:\n        x = x.replace(s, specials[s])\n    return x",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "e26fc7306d1822392118d97f7a00ca710fa52f23",
        "trusted": true
      },
      "cell_type": "code",
      "source": "import string\nfrom nltk import WordNetLemmatizer\nwnl = WordNetLemmatizer()\n\ndef CleanQuest(qlist):\n    print(\"Lowercasing\")\n    #lowercasing - necessary for paragram\n    qlist = qlist.apply(lambda x: \" \".join(x.lower() for x in x.split())) \n    \n    print(\"Replacing contractions, unknown chars & commonly misspelled words; removing punctuation\")\n    #remove contractions, replace unknown characters with known characters, replace commonly misspelled words, remove punctuation\n    qlist = qlist.apply(lambda x: Replc(x)) \n    \n    print(\"Lemmatisation of text\")\n     # Lemming\n    qlist = qlist.apply(lambda x: wnl.lemmatize(x))\n    \n    print(\"Removing punctuation\")\n    translator = str.maketrans('', '', string.punctuation)\n    qlist = qlist.apply(lambda x: x.translate(translator))\n    return qlist\n\ndef RemWord(qlist):\n    print(\"Removing 25 commonly occuring words\")\n    #remove commonly appearing words\n    qlist = qlist.apply(lambda x: \" \".join(x for x in x.split() if x not in list(comm_word.word)))\n    \n    print(\"Removing 25 rarely occuring words\")\n    #remove commonly appearing words\n    qlist = qlist.apply(lambda x: \" \".join(x for x in x.split() if x not in list(rare_word.word)))\n    return qlist",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "f151cfefb497c1d70d6651dd5bcd46efd5097424",
        "trusted": false
      },
      "cell_type": "code",
      "source": "print(\"Training:\")\nstart = time.time()\ntrain.question_text = CleanQuest(train.question_text)\nend = time.time()\ntle = (end- start) /60\nprint(f\" This process takes {tle} minutes\")\nprint(\"\\nTesting:\")\nstart = time.time()\ntest.question_text = CleanQuest(test.question_text)\nend = time.time()\ntle = (end- start) /60\nprint(f\" This process takes {tle} minutes\")\ndel ContDict, PunctDict,MisspellDict\ngc.collect()\ntime.sleep(10)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "9161201b596ff6487b48aaa5e45544e386675313",
        "trusted": false
      },
      "cell_type": "code",
      "source": "for row in train.question_text[0:10]:\n    print(row)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "e0df795a1734cfd729b8a3ee86231f81515b6251",
        "trusted": false
      },
      "cell_type": "code",
      "source": "comm_word = pd.Series(' '.join(train.question_text).split()).value_counts()[:25]\ncomm_word = pd.DataFrame({'word':comm_word.index, 'frequency':comm_word.values})\nrare_word = pd.Series(' '.join(train.question_text).split()).value_counts()[-25:]\nrare_word = pd.DataFrame({'word':rare_word.index, 'frequency':rare_word.values})",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "eb6ba3878ba74b035fe67dc135b4d1677eed1e9c",
        "trusted": false
      },
      "cell_type": "code",
      "source": "start = time.time()\ntrain.question_text = RemWord(train.question_text)\ntest.question_text = RemWord(test.question_text)\nend = time.time()\ntle = (end- start) /60\nprint(f\" This process takes {tle} minutes\")",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "0397cc8c483685f2ce7f1b8ecb579e772591389c",
        "trusted": false
      },
      "cell_type": "code",
      "source": "for row in train.question_text[0:5]:\n    print(row)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "6f7cfd1a5ed00959c17b14e12852d0a46954619f"
      },
      "cell_type": "markdown",
      "source": "## 4. Processing\nAfter cleaning the data, lets process the data:\n- Tokenize the data and convert the text to sequences.\n-  Pad sequences to ensure that all the sequences have the same shape.\n-  Assign varraibles max_feautures - vocaublary size, maxlen- arbitray value of 70, embed_size - size of embedding matrix.\n\n- Embeddings that can be used:\n    - GoogleNews-vectors-negative300 (i.e. w2vec) \n    - GLOVE embeddings\n    - Paragram embeddings\n    - Wiki News FastText"
    },
    {
      "metadata": {
        "_uuid": "04ff69aa82a2b6ac540afaa50f52f1bcc3836683",
        "trusted": true
      },
      "cell_type": "code",
      "source": "#load embedding locations\ng_embd = \"../input/embeddings/glove.840B.300d/glove.840B.300d.txt\"\npara_embd=  \"../input/embeddings/paragram_300_sl999/paragram_300_sl999.txt\"\nwikifast_embd = \"../input/embeddings/wiki-news-300d-1M/wiki-news-300d-1M.vec\"\n#w2v_embed =  \"../input/embeddings/GoogleNews-vectors-negative300/GoogleNews-vectors-negative300.bin\"\n\n## some constants \nmax_features = 100000 # how many unique words to use (i.e num rows in embedding vector)\nmaxlen = 70 #max number of words in a question to use\nembed_size = 300 #to use to create N x N embedding matrix",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "882604d4ff07d9cee6282d8786a5b1e31e55d02a",
        "trusted": true
      },
      "cell_type": "code",
      "source": "#Text Preprocessing\n# i.e Tokenize the sentences\ndef process(Traindata, Testdata): #function to tokenise on question list   \n    print(\"Step 1: Fill in any missing values\")\n    ## Step 1: fill in any missing values\n    xtrain = Traindata.question_text\n    ytrain = Traindata.target.values\n    Testquest = Testdata.question_text\n    #------------------------#\n    xtrain = xtrain.fillna(\"_##_\").values\n    Testquest = Testquest.fillna(\"_##_\").values\n    \n    print(\"Step 2: Tokenise using Trainquest\")\n    #Step 2: tokenise using Train\n    tokenizer = Tokenizer(num_words=max_features)\n    tokenizer.fit_on_texts(list(Traindata.question_text)) #train\n    \n    print(\"Step 3: Convert questions to sequences\")\n    #Step 3: convert questions to sequences\n    xtrain = tokenizer.texts_to_sequences(xtrain)\n    Testquest = tokenizer.texts_to_sequences(Testquest)\n\n    print(\"Step 4: Pad the sentences, i.e. get to same length\")\n    #Step 4: Pad the sentences, i.e. get to same length\n    xtrain = pad_sequences(xtrain, maxlen=maxlen)\n    Testquest = pad_sequences(Testquest, maxlen=maxlen)\n    \n    print(\"Done\")\n    #return train data (split later) and word index  \n    return xtrain, ytrain,Testquest, tokenizer.word_index",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "c1cfffd3162e2291d4544b892cb06335297ccccf",
        "trusted": true
      },
      "cell_type": "code",
      "source": "def getEmbedMatrix(EMBEDDING_FILE, Indx):\n    #1. open file\n    print(\"Step 1: Open embedding file\")\n    f = open(EMBEDDING_FILE, encoding=\"utf8\", errors='ignore') \n    \n    #2. embeddings index\n    print(\"Step 2: Get embeddings index\")\n    start = time.time()\n    def get_coefs(word,*arr): \n        return word, np.asarray(arr, dtype='float32')\n    embeddings_index = dict(get_coefs(*o.split(\" \")) for o in f if len(o)>100)\n    f.close()\n    print('Loaded %s word vectors.' % len(embeddings_index))\n    end = time.time()\n    tle = (end - start) //60\n    print(\" \"*3, \"Step 2 takes: \",tle , \"minutes\")\n    \n    #3. clear memory\n    print(\"Step 3: Clear memory\")\n    gc.collect() \n    \n    #4.create defualt embedding matrix\n    print(\"Step 4: Get default embedding matrix\")\n    #mean and sd of embedding weights\n    start = time.time()\n    all_embs = np.stack(embeddings_index.values())\n    emb_mean,emb_std = all_embs.mean(), all_embs.std()\n    embed_size = all_embs.shape[1]\n    \n    word_index = Indx\n    nb_words = min(max_features, len(word_index))\n    #embedding matrix all values same\n    embedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, embed_size)) \n    end = time.time()\n    tle = (end- start) //60\n    print(\" \"*3, \"Step 4 takes: \",tle , \"minutes\")\n        \n    #6. populate embedding matrix, ie. weight matrix\n    print(\"Step 5: Populate embedding matrix\")\n    start = time.time()\n    for word, i in tqdm(word_index.items(), total=len(word_index.items())):\n        if i >= max_features: continue \n        embedding_vector = embeddings_index.get(word)\n        if embedding_vector is not None: \n            embedding_matrix[i] = embedding_vector\n    end = time.time()\n    tle = (end- start) //60\n    print(\" \"*3, \"Step 5 takes: \",tle , \"minutes\")\n     \n    #Clear memory\n    del embeddings_index, all_embs, emb_mean, emb_std, nb_words,embedding_vector, word_index\n    gc.collect()\n    time.sleep(10)\n    return embedding_matrix",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "c13327a5a4625d9e8938715eb2faba5e3fc2d4df"
      },
      "cell_type": "markdown",
      "source": "## The following steps are done below:\n   - Process the training and testing data.\n   - Split the processed training data into training and validation sets for model fitting.\n   - Create the GLOVE, Paragram and WikiFastText embedding matrices."
    },
    {
      "metadata": {
        "_uuid": "4561044c2e5e237fbbcf65cf0d3ce92db185422e",
        "trusted": true
      },
      "cell_type": "code",
      "source": "#xtrain, ytrain, xval, yval, Xtest, Wd_Indx = process(train, test)\nprint(\"Processing data:\")\nstart = time.time()\nxtrain, ytrain,Xtest, Wd_Indx = process(train, test)\nend = time.time()\ntle = (end- start) /60\nprint(f\" This process takes {tle} minutes\")\nprint(f\"Train set is {xtrain.shape[0]} questions.\")",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "c413f5bdafeccac0864f2092bcce6d5160f035e6",
        "trusted": true
      },
      "cell_type": "code",
      "source": "start = time.time()\nGloveEmb_Matrix = getEmbedMatrix(g_embd,Wd_Indx)\nend = time.time()\ntle = (end- start) //60\nprint(f\"Creating Glove embedding matrix takes {tle} minutes\")",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "7cc54a3776b14b16f088324b764d7735c1448cf2",
        "trusted": true
      },
      "cell_type": "code",
      "source": "start = time.time()\nParaEmb_Matrix = getEmbedMatrix(para_embd,Wd_Indx)\nend = time.time()\ntle = (end- start) //60\nprint(f\"Creating Paragram embedding matrix takes {tle} minutes\")",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "7686033954dae32f168f9e671d25e66a1904a3ad",
        "trusted": true
      },
      "cell_type": "code",
      "source": "start = time.time()\nWiki_Matrix = getEmbedMatrix(wikifast_embd,Wd_Indx)\nend = time.time()\ntle = (end- start) //60\nprint(f\"Creating FastText embedding matrix takes {tle} minutes\")",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "92a25db6b663305ecb16046832d5a7f10c35ace0",
        "trusted": true
      },
      "cell_type": "code",
      "source": "#blending\nBlendedEmb_Matrix= np.concatenate((GloveEmb_Matrix, ParaEmb_Matrix, Wiki_Matrix) ,axis = 0) #Wiki_Matrix\nprint(np.shape(BlendedEmb_Matrix))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "ad503eb0f47e92ea8ea6582d157e0b13c3c7d7d1",
        "trusted": true
      },
      "cell_type": "code",
      "source": "del GloveEmb_Matrix, ParaEmb_Matrix, Wiki_Matrix\ngc.collect()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_kg_hide-input": true,
        "_uuid": "c545abc728cf3ce6c1fe068a43e4ea796f62a8e1",
        "trusted": true
      },
      "cell_type": "code",
      "source": "#Memory at : 8.8GB/14GB\n#clear memory\ndel start, end, tle, train, test\ngc.collect()\ntime.sleep(10)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_kg_hide-input": true,
        "_uuid": "9de0fc9e67beb0c1f27943a2f2d5e0c506e2a9fd",
        "trusted": true
      },
      "cell_type": "code",
      "source": "#clear memory\ndel Wd_Indx,train_test_split,getEmbedMatrix,process,nltk,pad_sequences,CleanQuest,word_tokenize\ngc.collect()\ntime.sleep(10)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "e017c3e93a2c01c134b192ec8249644a9895f453",
        "trusted": true
      },
      "cell_type": "code",
      "source": "#locals()\n%who\n#dir()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "5b1b7e0105132cad64480fdae3483e697b7b6281"
      },
      "cell_type": "markdown",
      "source": "## 5.  Final Model\nWe will train one model on several embeddings and use blending  to get a final prediction."
    },
    {
      "metadata": {
        "_uuid": "9a0a581f1c50e40896332ae1d924ea7ebbca6652",
        "trusted": true
      },
      "cell_type": "code",
      "source": "print(\"Is gpu available:\",(tf.test.is_gpu_available()))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_kg_hide-input": true,
        "_uuid": "8e466199664c3a50d0e39d193007da3ac3a7c615",
        "trusted": true
      },
      "cell_type": "code",
      "source": "epchs = 2\nbatchsz = 256\ndef trainMod(model, xtrain_data, ytrain_data,xval_data,yval_data):\n    print(\"  Fitting Model\")\n    model.fit(xtrain_data, ytrain_data,batch_size= batchsz,epochs = epchs,\n                  validation_data=(xval_data,yval_data))\n    #Predict\n    print(\"  Predicting\")\n    model_pred = model.predict(xval_data, verbose = 1)\n    print(\"  Finding Best Threshold\")\n    best_thresh = 0.5\n    best_score = 0.0\n    for thresh in np.arange(0.1, 0.501, 0.01):\n        thresh = np.round(thresh, 2)\n        score = f1_score(yval_data, (model_pred > thresh).astype(int))\n        if score > best_score:\n            best_thresh = thresh\n            best_score = score\n    #-------------------------------------------------------------------------------#\n    print(\"Best Threshold: {:.4f}\".format(best_thresh))\n    print(\"F1 Score: {:.4f}\".format(best_score))\n    model_pred = (model_pred > best_thresh).astype(int)\n    return model_pred, best_thresh, best_score",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_kg_hide-input": true,
        "_uuid": "31440ce47cacb8e3ac6501df0683c67ff1d03c67",
        "trusted": true
      },
      "cell_type": "code",
      "source": "def classifier(Embedding_Matrix):\n    inp = Input(shape=(maxlen,))\n    x = Embedding(max_features*3, embed_size, weights=[Embedding_Matrix],\n                  trainable = False,input_length=maxlen)(inp)\n    x = SpatialDropout1D(0.2)(x)\n    x = Bidirectional(CuDNNGRU(128, return_sequences=True))(x)\n    x = Bidirectional(CuDNNGRU(64, return_sequences=True))(x)\n    avg_pool = GlobalAveragePooling1D()(x)\n    max_pool = GlobalMaxPooling1D()(x)\n    conc = concatenate([avg_pool, max_pool])\n    conc = Dense(32, activation=\"relu\")(conc)\n    conc = Dropout(0.2)(conc)\n    outp = Dense(1, activation=\"sigmoid\")(conc)\n    model = Model(inputs=inp, outputs=outp)\n    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy']) \n    return model",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_kg_hide-output": true,
        "_uuid": "e121cc76261c468118e3285bccc22e9a3d61aba9",
        "scrolled": true,
        "trusted": true
      },
      "cell_type": "code",
      "source": "#Define Model\nblend_classifier = classifier(BlendedEmb_Matrix)\nblend_classifier.summary()\ngc.collect()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "4a47cb5e688a1d14ba23ef70e6c9b45805370a0e"
      },
      "cell_type": "code",
      "source": "#plot model\nfrom IPython.display import SVG\nfrom keras.utils.vis_utils import model_to_dot\nSVG(model_to_dot(blend_classifier, dpi = 50).create(prog='dot', format='svg'))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "2eb0d5a19667595f1382b78b51c7409ba4bbf205"
      },
      "cell_type": "markdown",
      "source": "This classification has a target imbalance problem; the sincere target is extremely larger than the insincere target. This means that the model will have high accuracy for the sincere target but low accuracy for the insincere target as there is too little information (hence a low f1 score).\n\nTo solve this we use StratifiedKfold sampling.\nStratification means that we sample seperately from each target class and allows the insincere target to be repeatedly sampled in each fold; it allows control of class ratio in each split."
    },
    {
      "metadata": {
        "_kg_hide-input": true,
        "_uuid": "e78e6ecd1cac84ac72395580b0347bda75a0e2db",
        "scrolled": false,
        "trusted": true
      },
      "cell_type": "code",
      "source": "#Stratified Kfold to deal with class imbalance\nstart = time.time()\nfrom sklearn.model_selection import StratifiedKFold\nprint(\"Use Stratified 5 Kfold to deal with class imbalance\")\nkfold = 5\nskf = StratifiedKFold(n_splits=kfold, random_state=12345)\nsplits = skf.split(xtrain, ytrain)\nval_pred = np.zeros(xtrain.shape[0])\ntest_pred = np.zeros(Xtest.shape[0])\n\nfor i, (train_index,val_index) in enumerate(splits):\n    print('\\n[Fold %d/%d]' % (i + 1, kfold))\n    x_train, x_val = xtrain[train_index], xtrain[val_index]\n    y_train, y_val = ytrain[train_index], ytrain[val_index]\n    # Train the model! \n    #blend_pred is prediction on val set, with threshold\n    blend_pred, blend_thresh, blend_score = trainMod(blend_classifier,\n                                                x_train, y_train,x_val,y_val) \n    val_pred[val_index] = blend_pred.reshape(-1)/ kfold\n    print(\"Predicting on Test Set\")\n    blend_testpred = blend_classifier.predict(Xtest, verbose = 1, batch_size = batchsz)\n    test_pred += blend_testpred.reshape(-1) / kfold\n\nend = time.time()\ntle = (end- start) //60\nprint(f\"Trainng model and predicting on test set takes {tle} minutes\")",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "311a0c0beaf37d66381d92cab8a23e5f2c94ec05",
        "trusted": true
      },
      "cell_type": "code",
      "source": "print(\"Best Threshold: {:.4f}\".format(blend_thresh))\nprint(\"Best F1 Score: {:.4f}\".format(blend_score))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "9abbe59dc536281f672b7875732dfd219d98ebd1",
        "trusted": true
      },
      "cell_type": "code",
      "source": "print(\"Classification Report\")\nprint(classification_report(y_val,blend_pred, target_names=[\"Sincere\", \"Insincere\"]))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_kg_hide-input": true,
        "_uuid": "79fbad1e0b304a9f9f0843c2f8074c40b4630fef",
        "trusted": true
      },
      "cell_type": "code",
      "source": "#Confusion Matrix\nfrom sklearn.metrics import confusion_matrix\nConMat = confusion_matrix(y_val,blend_pred)\n# Plot confusion_matrix\nfig, ax = plt.subplots(figsize=(15, 10))\nhm = sns.heatmap(ConMat, annot=True,xticklabels=[\"Sincere\", \"Insincere\"], center=0,\n                 cmap=sns.diverging_palette(100, 225,as_cmap=True),\n            yticklabels=[\"Sincere\", \"Insincere\"],annot_kws={\"size\": 15},fmt='g')\nsns.set(font_scale=1.4) #for label size\nplt.ylabel('Actual',fontsize=14)\nplt.xlabel('Predicted',fontsize=14)\nplt.title(\"Confusion Matrix\",fontsize=14)\nplt.show()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "8faf467a1d4179f92e1f45de7777bb04ac9111d0"
      },
      "cell_type": "markdown",
      "source": "## 7. Submission"
    },
    {
      "metadata": {
        "_uuid": "86bac3a068e26d2cbdfd3234580fb11acf068eea",
        "trusted": false
      },
      "cell_type": "code",
      "source": "#Submission\nYpred = (test_pred > blend_thresh).astype(int) #0.34\nsubmission = pd.DataFrame({'qid': np.asa     rray(np.asarray(qid)), \n                           'prediction': Ypred}, \n                          columns=['qid', 'prediction'])\nprint(submission.head())\nsubmission.to_csv(\"submission.csv\",index=False)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_kg_hide-input": true,
        "_uuid": "9bfb1b542543d907b0bf838824738cb1dcf0cc1d",
        "trusted": false
      },
      "cell_type": "code",
      "source": "compend = time.time()\ncomptime = (compend - compstart)\ncomptime  = np.round(comptime / 60,3)\nprint(f\"Running this notebok takes {comptime} minutes\")",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.6",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}